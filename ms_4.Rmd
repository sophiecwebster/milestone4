---
title: "Milestone 4"
author: "Sophie Webster"
date: "2/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(rvest)
library(tm)
library(wordcloud)
library(wordcloud2)
```
For this week's milestone, I decided to expand upon my past plots and dig into who is sending the messages and what they are saying. So, I first harvested the area codes out of the phone numbers column and scraped a website that contained a table that identified the state of each caller. My next steps here are to hopefully acquire information on student body demographics by state, so I can do per capita analysis here as well. Next, I also made a word cloud with the content from the messages, after making them into character strings. This highlights the most common words in the messages, after subtracting out stopwords ("a", "an", "the", etc.). My next goal is to learn how to do language processing and have a function determine whether or not messages are positive so it can sort through the messages.

Repo is available <a href="https://github.com/sophiecwebster/milestone4">here</a>.
```{r}
tntell <- read.csv("./textntell.csv", stringsAsFactors = FALSE) %>% select(Start, Tracker, Location, Mobile.Number, Comment)
```

```{r cursory plot}
tntell$arranged <- tntell$Location %>% fct_relevel("Annenberg", "Lowell", "Dunster", "Cabot", "Quincy", "Mather", "Pforzheimer", "Winthrop", "Currier", "Leverett", "Eliot", "Adams", "Hillel", "Kirkland", "FlyBy", "Dudley")

# for loop thru all locations? maybe make new frame for number of and folks per house
# tntell %>% filter(Location == "Dunster") %>% count()
# 
# for(i in unique(tntell$Location[1:16])){
#   filter(Location = "i") %>% count()
# }

per_cap <- data.frame(House = unique(tntell$Location), Student.Pop = c(371, 414, 476, 430, 408, 466, 381, 361, 1715, 388, NA, 403, 515, NA, 390, NA), Messages = c(143, 361, 255, 61, 143, 41, 163, 275, 429, 276, NA, 189, 73, NA, 19, NA)) %>% filter(!is.na(Student.Pop))

ggplot(tntell, aes(x = forcats::fct_rev(arranged))) + geom_bar(fill = "#f15b29") + coord_flip() + ggtitle("Who Sent HUDS the Most Messages in 2019?") + labs(y = "", x = "") + theme_light()
       
ggplot(per_cap, aes(x = reorder(House, Messages/Student.Pop), y = (Messages/Student.Pop))) + geom_col(fill = "#f15b29") + coord_flip() + ggtitle("Messages Per Capita By Harvard House") + labs(y = "", x = "") + theme_light() #+ labs(y = "Messages Per Capita")

# do comments per person
# crankiest students

# what about a "today's menu" section that scrapes site and says how people in the past have
# reacted to that entree
# wordcloud

# best rated/ranked quotes
# unique phone numbers versus folks in dhall
# positivity -- train language classifier!
# double check on publishing individual messages
# is this a representative sample of opinions?
```

```{r working with phone numbers, include=F}
# pulling out area codes 

tntell$area <- sub(".", "", tntell$Mobile.Number)
tntell$area <- substr(tntell$area,1,3) %>% as.double(tntell$area)

# mapping phone numbers

url <- paste0("https://www.areacodelocations.info/areacodelist.html")
h <- read_html(url)
html_text(h)
tab <- h %>% html_nodes("table")
tab <- tab %>% html_table() %>% as.data.frame()
tab <- tab[,c(1,2)]
tab$Area.code <- as.numeric(tab$Area.code)

tntell$House <- tntell$Location
located <- left_join(tntell[, c(1,2,4,5,6,7)], tab, by = c("area" = "Area.code"))
#mass = 236, ny = 236, ca = 236, tx = 177
# can either group these by region or just look at peeps in mass, ny, ca, tx
# also prob MA encompasses international students

```

```{r making plot}
located %>%
   ggplot(aes(Location)) + geom_bar() + coord_flip()
```

```{r word cloud, include=F}
comment <- as.character(tntell$Comment)
doc <- Corpus(VectorSource(comment))
doc_clean <- doc %>%
   tm_map(removeNumbers) %>%
   tm_map(removePunctuation) %>%
   tm_map(stripWhitespace)

doc2 <- tm_map(doc_clean, content_transformer(tolower))
final_doc <- tm_map(doc2, removeWords, stopwords("english"))

dtm <- TermDocumentMatrix(final_doc)
matrix <- as.matrix(dtm)
wordd <- sort(rowSums(matrix), decreasing = T)
df <- data.frame(word = names(wordd), freq=wordd)
```

```{r making word clouds}
# generate the word cloud

wordcloud(df$word, freq = df$freq, min.freq = 1, max.words = 200, random.order = F, rot.per=0.35, colors=brewer.pal(8,"Dark2"))

# use wordcloud2 to make more enhanced visualization; found HUDS' color palette 

wordcloud2(df, size = 1.3, color = rep_len(c("#f15b29", "#2bb673", "#BDD2FF", "#f3b204", "#F59187"), nrow(demoFreq)))
```

